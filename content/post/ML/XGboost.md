---
title: XGBoost 总结
date: 2019-03-20
tags:
    - machine learning
    - math
draft: true
categories:
    - Learn
summary: 做一些汇总，方便面试
---

## 简要介绍 XGBoost

- 基于 boosting 增强策略的加法模型, 每次迭代学习一颗 CART 决策树来拟合之前 t-1 树的预测结果与训练样本的真实值的残差
- 对 GBDT 进行了一系列优化， 如损失函数的二阶展开，目标函数引入正则项，支持并行和默认缺失值处理，在可扩展性和训练速度上有了巨大提升.

## 与 GBDT 异同

- **基分类器**： XGBoost 的基分类器不仅支持 CART 决策树，还支持线性分类器，此时，XGboost 相当于带 L1 和 L2 正则化的 Logistic 回归或者线性回归。
- **导数信息**： XGBoost 对损失函数进行了二阶展开， GBDT 只用了一阶导数信息， 并且 XGBoost 还支持自定义损失函数，只需供损失函数的一个，二阶可导。
- **正则项**： XGBoost的目标函数加入了正则项，相当于预剪枝， 是的模型不容易过拟合
- **列抽样**： XGBoost 支持列采样， 与随机森林类似， 防止过拟合
- **缺失值处理**： 对树中每个非叶子结点，XGBoost 可以自动学习他的默认分裂方向，如果某个样本的特征值缺失，会将其划入默认分支
- **并行化**： 特征维度的并行。 XGBoost 将每个特征按照特征值排好顺序，存储为快结构，分裂结点可以采用多线程并行查找每个特征的最佳分裂结点，极大提升训练速度

## XGBoost 防止过拟合方法

  -  目标函数添加正则项： 叶子节点个数 + 叶子节点权重的 L2 正则化
  - 列抽样，只使用一部分特征
  - 子采样， 每轮统计不使用全部样本，让算法更加保守
  - shrinkage: 学习率或者步长， 给后面的训练留出更多的学习空间

## XGBoost 如何处理缺失值
   
   - 在特征 K 上寻找最佳分裂点时， 不会对特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本进行特征值遍历，通过这个技巧减少为稀疏特征寻找 split point 的开销
   - 为了保证完备性，将该特征值 missing 的样本分别分配到 做叶子节点和右叶子节点， 两种情形计算一遍后，选择分裂后增益最大的方向，作为预测时特征值缺失样本的默认分支方向
   - 如果在训练没有缺失值而在预测中出现缺失值，自动将缺失值划分方向放到右子节点

## XGBoost 叶子节点的权重如何计算

  目标函数
  $$ Obj^{(t)} = \sum_{j=1}^T[G_jw_j + \frac12(H_j + \lambda) w_j^2] +\gamma T$$
  对目标函数求导， 则可得每个叶子节点的权重
  $$w_j^* = - \frac{G_j}{H_j + \lambda}$$
  计算得 第 $t$ 棵树带来的最小损失函数 (训练损失 + 正则化损失)
  $$Obj = - \frac12 \sum_{j=1}^{T} \frac{G_j^2}{H_j + \lambda} + \gamma T$$

## XGBoost 中一棵树停止生长的条件

- 当引入的一次分裂带来的增益 Gain < 0 时， 放弃当前的分裂
- 当树达到最大深度， 停止建树
- 引入一次分裂后，重新计算生成的左，右叶子节点的样本权重和。如果任意一个叶子节点的权重低于某一个阈值，放弃此分裂，需要设定一个最小样本权重和， 指如果一个叶子节点包含样本数量太小时放弃分裂，防止树分的太细。

## RF 与 GBDT 区别

相同点：
  - 都是由多棵树组成，最终的结果是由多棵树共同决定
不同点：
  - 集成学习： RF 属于 Bagging 思想， GBDT 属于boosting思想
  - 偏差-方差权衡： RF 不断降低模型的方差，而 GBDT 不断降级模型的偏差
  - 训练样本： RF 每次迭代的样本是从全部训练集有放回的抽样而形成，GBDT 每次使用全部样本
  - 并行性： RF 的树可以并行生成，而GBDT只能顺序生成
  - 最终结果： RF 最终结果是多棵树进行表决， GBDT 是加权融合
  - 数据敏感性： RF 对异常值不敏感，而 GBDT 对异常值比较敏感
  - 泛化能力： RF 不容易过拟合

## XGBoost 如何处理数据不平衡问题

对弈数据不平衡问题， 主要有两种方法
1. 如果采样AUC 来评估模型性能， 通过设置 `scale_pos_weight` 来平衡正样本和负样本的权重, 增大少量样本的权重。
2. 如果在意概率，不能重新平衡数据集，应该设置 `max_delta_step` 为一个有效的数字帮助收敛

## 比较 LR 和 GBDT

  - LR 是线性模型， 解释性强， 容易并行化，但学习能力有限，需要大量的特征工程
  - GBDT 是非线性模型， 具有天然的特征组合优势，特征表达能力强，树与树之间无法并行训练，模型容易过拟合
  - 高维特征下，LR 的效果一般比 GBDT 要好， 因为在高维空间中，数据集会是线性可分得

## XGBoost 如何进行剪枝

  - 在目标函数中加入正则项，利用叶子节点的数目和叶子节点权重的平方， 控制树的复杂度
  - 在节点分裂时，定义了一个阈值，如果分裂后的目标函数增益小于阈值，则不分裂
  - 在引入一次分裂后， 重新计算生成的左，右叶子节点的样本权重和，如果任意一个叶子节点的样本权重低于某一个阈值(最小样本权重和)， 放弃。
  - XGboost 从顶到底建立树到最大深度，再从底到顶反向检查是否有不满足分裂条件的节点，进行剪枝

## XGBoost 如何选择最佳分裂节点

- 在训练前将特征按照特征值进行排序，存储为 block 结构，以后在节点分裂后可以重复使用
- 利用多个线程计算每个分裂节点的最佳值，根据每次分裂后产生的增益，选择增益最大的特征的特征值为最佳分裂点
- 对特征排序后仅选择常数个数分裂位置作为候选分裂点，提高计算效率

## XGBoost 如何评价特征的重要性

- weight: 特征在所有树种被用作分割样本的特征的总次数
- gain: 该特征在其出现的所有树中产生的平均增益
- cover: 该特征在其出现过的所有树中的平均覆盖范围 (有多少样本经过该特征分割到两个子节点)

## XGBoost 参数调优的一般步骤

定义初始变量

  - max_depth = 5
  - min_child_weight = 1;
  - gamma = 0;
  - subsmaple, colsample_bytree = 0.8
  - scale_pos_weight = 1

1. 确定 learning_rate 和 estimator 数量
2. max_depth 和 min_child_weight
    每个参数对输出结果影响较大， 受限将这两个参数设置为较大的值，通过迭代的方式不断修正， 缩小范围， 一个节点分裂后，所有子节点的权重之和应该大于该阈值，该叶子节点才可以划分
3. gamma
    最小划分损失 min_split_loss, 0.1 ~ 0.5, 对于一个叶子节点，当对他采取划分之后，损失函数降低值的阈值
    如果大于该阈值，则该叶子节点可以继续划分
4. subsample, colsample_bytree(0.6 ~ 0.9)
  - subsample 对训练样本的采样比例
  - colsample_bytree 对特征的采样比例
5. 正则化参数
   - alpha L1正则化系数
   - lambda L2 正则化系数
6. 降低学习率， 同时增加树的数量
  
## XGBoost 如果出现过拟合怎么解决
- 直接控制模型的复杂度
  max_depth, min_child_wegiht, gamma
- 增加随机性， subsample, colsample_bytree
- 减小 learning_rate, 增加 estimator 参数

## XGBoost 对模型的缺失值不敏感

对缺失值的特征，一般处理方法是， 用出现次数最多的特征填充，或者中位数和均值填充。

如SVM, KNN, 涉及到了对样本距离的度量，缺失值如果处理不当，最终导致模型的预测效果较差
树模型对缺失值的敏感度低， 一棵树中每个节点分裂时， 寻找某个特征的最佳分裂点，不考虑存在特征值缺失的样本，样本的特征值缺失，对寻找最佳分割点的影响不是很大。