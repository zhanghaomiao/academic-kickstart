---
title: 矩阵求导
date: 2019-10-01
tags:
    - machine learning
    - math
categories:
    - Learn
summary: 计算学习中涉及到很多矩阵求导运算， 这里给出简单的矩阵求导
---

## 定义

- 标量 $f$ 对矩阵 $\mathbf{X}$ 的导数， 定义为
  $$\frac{\partial f}{\partial \mathbf{X}} = \left[\frac{\partial f}{\partial X_{ij}}\right]$$
  在求导时不宜拆开矩阵， 需要找到一个整体的算法 
- 一元微积分中的导数与微分的关系有
  $$df = f'(x) dx$$
- 多元的导数与微分的关系
  $$df = \sum_{i=1}^{n}\frac{\partial f}{\partial x_i} dx_i = \frac{\partial f}{\partial \mathbf{x}}^{t} d\mathbf{x}$$
- 矩阵导数与微分建立联系：
  $$d f=\sum_{i=1}^{m} \sum_{j=1}^{n} \frac{\partial f}{\partial X_{i j}} d X_{i j}=\operatorname{tr}\left(\frac{\partial f^{T}}{\partial X} d X\right)$$
  $tr$ 表示方阵对角元素之和
  
## 运算法则

- 加减法 $d(\mathbf{X}\pm \mathbf{Y}) = d\mathbf{X} \pm d\mathbf{Y}$
- 乘法 $d(\mathbf{XY}) = (d\mathbf{X})\mathbf{Y} + \mathbf{X}d\mathbf{Y}$ 
- 转置 $d(\mathbf{X}^T) = (d\mathbf{X})^T$
- [参考 matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)

## 计算

建立矩阵与微分联系时， 在求出左侧的微分 $df$, 需要写出右侧形式的导数
  $$df = tr\left(\frac{\partial f}{\partial \mathbf{X}}^T d\mathbf{X}\right)$$
  
- 标量上迹 $a = tr(a)$
- 转置 $tr(A^T) = tr(A)$
- 加减法 $tr(A\pm B) = tr(A) \pm tr(B)$
- 矩阵乘法交换 $tr(AB) = tr(BA)$
- 矩阵乘法/逐元乘法交换 $\text{tr}(A^T(B \odot C))=\text{tr}((A\odot B)^TC)$, 其中 A, B, C 尺寸相同

## Example

- Linear Regression $l = || \mathbf{Xw - y}||^2$,  solve $\mathbf{w}, \mathbf{y}$ is a $m\times 1$ vector, $\mathbf X$ has the shape $m\times n$, and $w$'s shape is $n\times 1$, and the $l$ is a scalar

    In this example, we need to solve a scalar's derivate with respect to a vector. As the defintion, we can't calcuate the derivate of a vector or a matrix directly. Instead, we need to calcuate the differential of a matrix.
    $$\begin{aligned}l &= \mathbf{(Xw - y)}^T(\mathbf{Xw - y}) \\  
        dl &= (X d \boldsymbol{w})^{T}(X \boldsymbol{w}-\boldsymbol{y})+(X \boldsymbol{w}-\boldsymbol{y})^{T}(X d \boldsymbol{w})\\
     \end{aligned}$$
    As the first and second form is a dot product between two vector, so we can get
    $$dl = 2(X \boldsymbol{w}-\boldsymbol{y})^{T} \boldsymbol{X} d \boldsymbol{w}$$
    The differential form of $dl$
    $$dl = \frac{\partial l}{\partial \mathbf{w}}^T d\mathbf{w}$$
    The formula transforms to
    $$\frac{\partial l}{\partial \boldsymbol{w}}=2 X^{T}(X \boldsymbol{w}-\boldsymbol{y})$$
    Set the partial form to $0$, we could get
    $$\mathbf{X^TXw} = \mathbf{X^Ty}$$
    and the $w$ will be
    $$\mathbf{w = (X^TX)^{-1}X^Ty}$$
  
## Reference

- [矩阵求导术（上）知乎](https://zhuanlan.zhihu.com/p/24709748)  
- [matrix cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)